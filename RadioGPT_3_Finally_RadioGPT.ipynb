{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33EWi1Bq4kM6"
      },
      "source": [
        "![RadioGPT Banner](https://openfileserver.chloelavrat.com/workshops/RadioGPT/assets/radiogpt-banner.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Nn_cKD4g_G"
      },
      "source": [
        "> üí° **PLEASE CONNECT USING A GPU SESSION FOR MORE COMPUTE POWER** :\n",
        ">\n",
        "> `Runtime > Change runtime type > T4 GPU > Save`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "jU92rWwZ8re3"
      },
      "outputs": [],
      "source": [
        "#@title Initialize the notebook\n",
        "!git clone https://github.com/chloelavrat/RadioGPT.git > /dev/null 2>&1\n",
        "!cd RadioGPT && git checkout clavrat/first-version > /dev/null 2>&1\n",
        "!pip install torch datasets tqdm transformers > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zehV3Qz5yjo"
      },
      "source": [
        "# üìÄ Dataset Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHMxt75R6JXc",
        "outputId": "bb23afb6-6842-48a4-a8aa-db2f4cde326e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "Dataset downloaded!\n",
            "Loaded 110368 conversations\n",
            "Maximum sequence length: 64\n",
            "Dataset loaded !\n"
          ]
        }
      ],
      "source": [
        "#@title Select your radio dataset\n",
        "#@markdown Please select your favorit Radio Station ! Then run the cell to load the dataset üå±\n",
        "import os\n",
        "import subprocess\n",
        "from RadioGPT.gptmodel.core.dataset import AlpacaDataset\n",
        "\n",
        "radio_station = 'France Inter' # @param [\"France Inter\", \"Mouv‚Äô\", \"France Culture\"]\n",
        "\n",
        "def download_dataset(url, destination):\n",
        "    print(\"Downloading dataset...\")\n",
        "    os.makedirs(\"dataset\", exist_ok=True)\n",
        "    subprocess.run([\"wget\", url, \"-O\", destination])\n",
        "    print(\"Dataset downloaded!\")\n",
        "\n",
        "base_url = \"https://openfileserver.chloelavrat.com/workshops/RadioGPT/dataset/\"\n",
        "\n",
        "if radio_station == 'France Inter':\n",
        "  block_size = 64\n",
        "  file = \"Acquiesce_data_110k_instructions.json\"\n",
        "  destination = \"dataset/inter.json\"\n",
        "\n",
        "if radio_station == 'Mouv‚Äô':\n",
        "  print(\"bli\")\n",
        "  # load Mouv‚Äô dataset\n",
        "\n",
        "if radio_station == 'France Culture':\n",
        "  print(\"bli\")\n",
        "  # load France Culture dataset\n",
        "\n",
        "download_dataset(base_url+file, destination)\n",
        "dataset = AlpacaDataset(destination, block_size)\n",
        "print(\"Dataset loaded !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zND_ANv351e3"
      },
      "source": [
        "# üß† Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FE4E1o9-cW9",
        "outputId": "859d278b-fa4b-4bb8-a1c4-d1bc5551ef79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model...\n",
            "Loading model...\n",
            "Model loaded !\n",
            "You will use your cpu\n",
            "Total number of parameters   83.1M\n"
          ]
        }
      ],
      "source": [
        "#@title Loading ...\n",
        "# Load RadioGPT's checkpoint :)\n",
        "from RadioGPT.gptmodel.core.model import GPTlite\n",
        "from RadioGPT.gptmodel.core.utils import load_model\n",
        "import os, subprocess, torch\n",
        "# get device\n",
        "device = (\n",
        "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
        "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
        "    torch.device(\"cpu\")\n",
        ")\n",
        "\n",
        "# downloading model\n",
        "print(\"Downloading model...\")\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "subprocess.run([\"wget\", \"https://openfileserver.chloelavrat.com/workshops/RadioGPT/models/model_gpt_chat_best.pth\", \"-O\", \"models/model_gpt_chat_best.pth\"])\n",
        "\n",
        "def load_model(model_path, device, config):\n",
        "    # Load the model checkpoint\n",
        "    checkpoint = torch.load(\n",
        "        model_path, map_location=device, weights_only=False)\n",
        "\n",
        "    model = GPTlite(config)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "block_size = 64\n",
        "n_embd = 512\n",
        "n_head = 8\n",
        "n_layer = 10\n",
        "dropout = 0.2\n",
        "\n",
        "config = {\n",
        "    'context_size': block_size,\n",
        "    'vocab_size': dataset.vocab_size,\n",
        "    'embedding_dim': n_embd,\n",
        "    'num_heads': n_head,\n",
        "    'num_layers': n_layer,\n",
        "    'dropout': dropout\n",
        "}\n",
        "\n",
        "# Loading model in memory\n",
        "print(\"Loading model...\")\n",
        "model = load_model(\"models/model_gpt_chat_best.pth\", device, config)\n",
        "model = model.to(device)\n",
        "print(\"Model loaded !\")\n",
        "\n",
        "# Get the total number of parameters in the model\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"You will use your {device}\")\n",
        "print(f\"Total number of parameters   {total_params / 1e6:.1f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OD2BWvy566z"
      },
      "source": [
        "# ‚õ≥Ô∏è RadioGPT Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hAf1VVuc-n2U"
      },
      "outputs": [],
      "source": [
        "# training parameters\n",
        "learning_rate = 1e-3\n",
        "epochs = 1000\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "86YPl1zD-oP0"
      },
      "outputs": [],
      "source": [
        "#@title Casual Training Loop\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1, betas=(0.9, 0.999))\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "# Trinaing\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Let's fry some eggs!! (your loss should be less than 5, restart cell if not...)\")\n",
        "print(\"-\" * 5)\n",
        "\n",
        "# Training loop with tqdm progress bar\n",
        "pbar = tqdm(range(epochs), desc=\"Training\", ncols=120)\n",
        "\n",
        "# Trining Loop\n",
        "for steps in pbar:\n",
        "    # Evaluation loop\n",
        "    @torch.no_grad()\n",
        "    def eval_loss():\n",
        "        # Get a batch of validation data\n",
        "        idx, targets = dataset.get_batch(batch_size)\n",
        "        idx, targets = idx.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits, loss = model(idx, targets)\n",
        "\n",
        "        # Print evaluation loss on the right side of the tqdm bar\n",
        "        pbar.set_postfix(eval_loss=f\"{loss.item():.2f}\")\n",
        "        return loss\n",
        "\n",
        "    # Get a batch of training data\n",
        "    idx, targets = dataset.get_batch(batch_size)\n",
        "    idx, targets = idx.to(device), targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(idx, targets)\n",
        "\n",
        "    # Backward pass with gradient scaling\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # Update the tqdm description with loss value\n",
        "    if steps % 100 == 0:\n",
        "        pbar.write(f\" Eval {eval_loss().item():.2f}\")\n",
        "\n",
        "    pbar.set_postfix(loss=f\"{loss.item():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSlusiv-6FlK"
      },
      "source": [
        "# üé® Let's Play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TWhgbWHa56Xc"
      },
      "outputs": [],
      "source": [
        "#@title Selected prompt\n",
        "\n",
        "# play with the model\n",
        "def generate_response(model, dataset, prompt, device, max_new_tokens):\n",
        "    model.eval()\n",
        "    # Encode the prompt\n",
        "    input_tensor = dataset.encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate text using the model's generate method\n",
        "    with torch.no_grad():\n",
        "        generated_indices = model.generate(input_tensor, max_new_tokens)\n",
        "        generated_text = dataset.decode(generated_indices[0].tolist())\n",
        "\n",
        "    # Return only the newly generated part (after the prompt)\n",
        "    return generated_text[len(prompt):]\n",
        "\n",
        "\n",
        "prompt = 'Pr√©pare une recette de p√¢tes √† la carbonara.' # @param [\"Pr√©pare une recette de p√¢tes √† la carbonara.\", \"Quel est l'√©l√©ment chimique avec le num√©ro atomique 29 ?\", \"R√©dige un court paragraphe sur le th√®me de l'amiti√© et de la confiance.\"]\n",
        "max_new_tokens = 106 # @param {type:\"slider\", min:5, max:500, step:1}\n",
        "\n",
        "\n",
        "prompt = f\"Question: {prompt}\\nAnswer:\"\n",
        "prompt = prompt + generate_response(model, dataset, prompt, device, max_new_tokens)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ja7k1zxP_Su9"
      },
      "outputs": [],
      "source": [
        "#@title Let's prompt it!\n",
        "\n",
        "# play with the model\n",
        "def generate_response(model, dataset, prompt, device, max_new_tokens):\n",
        "    model.eval()\n",
        "    # Encode the prompt\n",
        "    input_tensor = dataset.encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate text using the model's generate method\n",
        "    with torch.no_grad():\n",
        "        generated_indices = model.generate(input_tensor, max_new_tokens)\n",
        "        generated_text = dataset.decode(generated_indices[0].tolist())\n",
        "\n",
        "    # Return only the newly generated part (after the prompt)\n",
        "    return generated_text[len(prompt):]\n",
        "\n",
        "\n",
        "prompt = 'D√©cris les diff√©rences entre le mod√®le GPT-2 et le mod√®le GPT-3.' # @param {type:\"string\"}\n",
        "max_new_tokens = 180 # @param {type:\"slider\", min:5, max:500, step:1}\n",
        "\n",
        "\n",
        "prompt = f\"Question: {prompt}\\nAnswer:\"\n",
        "prompt = prompt + generate_response(model, dataset, prompt, device, max_new_tokens)\n",
        "print(prompt)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
