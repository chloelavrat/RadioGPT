{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYkKBU7vbDsh"
      },
      "source": [
        "![GPT-Chat Banner](https://openfileserver.chloelavrat.com/workshops/RadioGPT/assets/gpt-chat-banner.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP0UD_RNa_1t"
      },
      "source": [
        "> üí° **PLEASE CONNECT USING A GPU SESSION FOR MORE COMPUTE POWER** :\n",
        ">\n",
        "> `Runtime > Change runtime type > T4 GPU > Save`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxH_dRXIdP1W"
      },
      "source": [
        "# üéØ Objective\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Mr4vJpgdCg"
      },
      "source": [
        "The goal of this second part of the workshop is to explore how to interact with and experiment using a pre-trained large language model. Through hands-on experience with a small but still larger than previously pre-trained model (83M parameters). In this section you will discover:\n",
        "\n",
        "- How to load and work with a pre-trained language model.\n",
        "- The structure of chatbot datasets and how to leverage them for generating responses.\n",
        "- The effects of adjusting model parameters like temperature, prompt design, and token limits.\n",
        "- The challenges of training and using very small models in real-world applications, including biases and overfitting.\n",
        "- How to explore and critique the model's responses to identify areas for improvement.\n",
        "\n",
        "By the end of this phase, you will have experimented with generating text, gained insight into model behavior, and understood the trade-offs involved in working with smaller models. üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okFAiCfIh-ea"
      },
      "source": [
        "# üé¨ Notebook initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J33Y1HS6iN7T"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/chloelavrat/RadioGPT.git > /dev/null 2>&1\n",
        "!cd RadioGPT && git checkout clavrat/first-version > /dev/null 2>&1\n",
        "!pip install torch datasets tqdm transformers tiktoken> /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbuK0X7HdU6R"
      },
      "source": [
        "# üìÄ Dataset Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yST8NXY1irv_"
      },
      "source": [
        "The French Alpaca Dataset contains **110,368 instruction-based examples** in French in the Alpaca format. The dataset was specifically created to fine-tune general language models for tasks like question-answering and text generation.\n",
        "\n",
        "**Here‚Äôs the twist:** this dataset was entirely generated by GPT-3.5-turbo itself! In other words, an LLM was used to create data that would train... another LLM. It's a fascinating example of AI systems helping to improve their own performance, all while producing a diverse set of French instructions that can be used for fine-tuning and model experimentation.\n",
        "\n",
        "üîó [Explore the dataset on Hugging Face](https://huggingface.co/datasets/jpacifico/French-Alpaca-dataset-Instruct-110K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXOfvAEZiofA",
        "outputId": "4f3b9913-c4f5-4bbc-c0fd-66805e013614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-14 18:13:55--  https://openfileserver.chloelavrat.com/workshops/RadioGPT/dataset/Acquiesce_data_110k_instructions.json\n",
            "Resolving openfileserver.chloelavrat.com (openfileserver.chloelavrat.com)... 149.202.72.149\n",
            "Connecting to openfileserver.chloelavrat.com (openfileserver.chloelavrat.com)|149.202.72.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51958855 (50M) [application/json]\n",
            "Saving to: ‚Äòdataset/Acquiesce_data_110k_instructions.json‚Äô\n",
            "\n",
            "dataset/Acquiesce_d 100%[===================>]  49.55M  8.63MB/s    in 7.2s    \n",
            "\n",
            "2024-12-14 18:14:03 (6.92 MB/s) - ‚Äòdataset/Acquiesce_data_110k_instructions.json‚Äô saved [51958855/51958855]\n",
            "\n",
            "Dataset downloaded\n"
          ]
        }
      ],
      "source": [
        "#@title Download French Alpaca dataset\n",
        "!mkdir dataset\n",
        "!wget https://openfileserver.chloelavrat.com/workshops/RadioGPT/dataset/Acquiesce_data_110k_instructions.json -O dataset/Acquiesce_data_110k_instructions.json\n",
        "print(\"Dataset downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r17mAOJMh_bl",
        "outputId": "40038839-46ae-4a66-8aaa-05a9b793ea72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 453174 blocks\n",
            "Maximum sequence length: 64\n",
            "Vocabulary size   :  50257\n",
            "Batch token       : \n",
            " (tensor([24361,    25,   371, 20954,  2454,   443, 48659,   660,   424,   452,\n",
            "          415,   551,  2026,   285,  1747,    13,  1004,  4767,   270,  9005,\n",
            "          390,  3738, 42722,   390,  9281,    12,  3109,   929,  2634,   563,\n",
            "         1556,   555,   542,    68,  6722,  2350, 45567,  3444, 38599,   300,\n",
            "            6, 10034, 32177,   288,     6,   403,  4273,   270, 19716, 45567,\n",
            "          627,  2654,   473,  1410, 14064,   660, 12797, 39349,   288,     6,\n",
            "         2306,   411,   285]), tensor([   25,   371, 20954,  2454,   443, 48659,   660,   424,   452,   415,\n",
            "          551,  2026,   285,  1747,    13,  1004,  4767,   270,  9005,   390,\n",
            "         3738, 42722,   390,  9281,    12,  3109,   929,  2634,   563,  1556,\n",
            "          555,   542,    68,  6722,  2350, 45567,  3444, 38599,   300,     6,\n",
            "        10034, 32177,   288,     6,   403,  4273,   270, 19716, 45567,   627,\n",
            "         2654,   473,  1410, 14064,   660, 12797, 39349,   288,     6,  2306,\n",
            "          411,   285,   623]))\n",
            "Batch decoded     : \n",
            " Question: R√©sume le texte suivant en 50 mots. Le Petit Prince de Antoine de Saint-Exup√©ry est un conte philosophique qui raconte l'histoire d'un petit prince qui quitte sa plan√®te pour explorer d'autres m\n"
          ]
        }
      ],
      "source": [
        "#@title Load Alpaca dataset\n",
        "from RadioGPT.gptmodel.core.dataset import AlpacaDataset\n",
        "\n",
        "block_size = 64 # you remember, \"sets the length of input sequences the model processes at once\"\n",
        "\n",
        "# Dataset\n",
        "dataset = AlpacaDataset(\"dataset/Acquiesce_data_110k_instructions.json\", block_size)\n",
        "\n",
        "print(\"Vocabulary size   : \", dataset.vocab_size)\n",
        "print(\"Batch token       : \\n\", dataset.__getitem__(42))\n",
        "print(\"Batch decoded     : \\n\", dataset.decode(dataset.__getitem__(42)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrPEcVz_1HJM"
      },
      "source": [
        "# üß† Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ckxHZ74na16",
        "outputId": "023bc3f2-f855-441b-b518-9570a34fb748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model...\n",
            "Loading model...\n",
            "Model loaded !\n",
            "You will use your cuda\n",
            "Total number of parameters   83.1M\n"
          ]
        }
      ],
      "source": [
        "#@title Loading ...\n",
        "# Load RadioGPT's checkpoint :)\n",
        "from RadioGPT.gptmodel.core.model import GPTlite\n",
        "from RadioGPT.gptmodel.core.utils import load_model\n",
        "import os, subprocess, torch\n",
        "# get device\n",
        "device = (\n",
        "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
        "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
        "    torch.device(\"cpu\")\n",
        ")\n",
        "\n",
        "# downloading model\n",
        "print(\"Downloading model...\")\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "subprocess.run([\"wget\", \"https://openfileserver.chloelavrat.com/workshops/RadioGPT/models/model_gpt_chat_best.pth\", \"-O\", \"models/model_gpt_chat_best.pth\"])\n",
        "\n",
        "def load_model(model_path, device, config):\n",
        "    # Load the model checkpoint\n",
        "    checkpoint = torch.load(\n",
        "        model_path, map_location=device, weights_only=False)\n",
        "\n",
        "    model = GPTlite(config)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "block_size = 64\n",
        "n_embd = 512\n",
        "n_head = 8\n",
        "n_layer = 10\n",
        "dropout = 0.2\n",
        "\n",
        "config = {\n",
        "    'context_size': block_size,\n",
        "    'vocab_size': dataset.vocab_size,\n",
        "    'embedding_dim': n_embd,\n",
        "    'num_heads': n_head,\n",
        "    'num_layers': n_layer,\n",
        "    'dropout': dropout\n",
        "}\n",
        "\n",
        "# Loading model in memory\n",
        "print(\"Loading model...\")\n",
        "model = load_model(\"models/model_gpt_chat_best.pth\", device, config)\n",
        "model = model.to(device)\n",
        "print(\"Model loaded !\")\n",
        "\n",
        "# Get the total number of parameters in the model\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"You will use your {device}\")\n",
        "print(f\"Total number of parameters   {total_params / 1e6:.1f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBsILJFqwgEI"
      },
      "source": [
        "As you can see below this model version features a lot more parameters resulting in a larger model of **83.1M** Parameters.\n",
        "\n",
        "Our model has **83.1 million parameters (83.1M)**, which is considered quite small compared to larger GPT models like GPT-2 and GPT-3. Here's a quick comparison:\n",
        "\n",
        "* **GPT-2 Small:** 117M parameters\n",
        "* **GPT-2 Medium:** 345M parameters\n",
        "* **GPT-2 Large:** 762M parameters\n",
        "* **GPT-3:** 175B parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA1PAVB9dgKh"
      },
      "source": [
        "# üé® Let's Play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rXUIET2xEyz",
        "outputId": "c69d8240-282a-4dc4-cc1a-da742b3849dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Pr√©pare une recette de p√¢tes √† la carbonara.\n",
            "Answer: Ingr√©dients : - 400 g de spaghetti - 150 g de pancetta - 4 ≈ìufs - 150 g de guanciale - 100 g de pecorino r√¢p√© - Pelerocke vanille\n",
            "\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "#@title Selected prompt\n",
        "\n",
        "# play with the model\n",
        "def generate_response(model, dataset, prompt, device, max_new_tokens):\n",
        "    model.eval()\n",
        "    # Encode the prompt\n",
        "    input_tensor = dataset.encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate text using the model's generate method\n",
        "    with torch.no_grad():\n",
        "        generated_indices = model.generate(input_tensor, max_new_tokens)\n",
        "        generated_text = dataset.decode(generated_indices[0].tolist())\n",
        "\n",
        "    # Return only the newly generated part (after the prompt)\n",
        "    return generated_text[len(prompt):]\n",
        "\n",
        "\n",
        "prompt = 'Pr√©pare une recette de p√¢tes √† la carbonara.' # @param [\"Pr√©pare une recette de p√¢tes √† la carbonara.\", \"Quel est l'√©l√©ment chimique avec le num√©ro atomique 29 ?\", \"R√©dige un court paragraphe sur le th√®me de l'amiti√© et de la confiance.\"]\n",
        "max_new_tokens = 106 # @param {type:\"slider\", min:5, max:500, step:1}\n",
        "\n",
        "\n",
        "prompt = f\"Question: {prompt}\\nAnswer:\"\n",
        "prompt = prompt + generate_response(model, dataset, prompt, device, max_new_tokens)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WwcfJSUs7hr",
        "outputId": "8c7bda41-7a5c-4d61-b794-143c6a3584d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: D√©cris les diff√©rences entre le mod√®le GPT-2 et le mod√®le GPT-3.\n",
            "Answer: Le mod√®le GPT-2 est un mod√®le de langage g√©n√©r√© d√©velopp√© par OpenAI. Il est basse terme de gestion du temps il ferais parcour 10¬∞C√¥ti du pays de ville extra Texas par le temps.\n",
            "Answer: Appreneiffelques endives du genre de nombre impair de sport√¥ti endorphives pas endantes du pays prochain nombre ni cultures divers. Par extra pas); scan n extrautes de nombreonde par men ville √† 10 heures animives du nombre de sport√¥tcles par men numvernement nature num Madrid.\n",
            "Answer: On the humanien nombre de vol extra termes a men par population nombre de nombreux pays participants plus men par le plus haut santes\n"
          ]
        }
      ],
      "source": [
        "#@title Let's prompt it!\n",
        "\n",
        "# play with the model\n",
        "def generate_response(model, dataset, prompt, device, max_new_tokens):\n",
        "    model.eval()\n",
        "    # Encode the prompt\n",
        "    input_tensor = dataset.encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate text using the model's generate method\n",
        "    with torch.no_grad():\n",
        "        generated_indices = model.generate(input_tensor, max_new_tokens)\n",
        "        generated_text = dataset.decode(generated_indices[0].tolist())\n",
        "\n",
        "    # Return only the newly generated part (after the prompt)\n",
        "    return generated_text[len(prompt):]\n",
        "\n",
        "\n",
        "prompt = 'D√©cris les diff√©rences entre le mod√®le GPT-2 et le mod√®le GPT-3.' # @param {type:\"string\"}\n",
        "max_new_tokens = 180 # @param {type:\"slider\", min:5, max:500, step:1}\n",
        "\n",
        "\n",
        "prompt = f\"Question: {prompt}\\nAnswer:\"\n",
        "prompt = prompt + generate_response(model, dataset, prompt, device, max_new_tokens)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWkI4kJpycdd"
      },
      "source": [
        "# ‚ú® **Conclusion**\n",
        "\n",
        "This exercise demonstrates both the potential and the limitations of working with smaller models. Despite having only **83.1M parameters**, the model is able to generate coherent sentence structures. However, as seen in the example, its responses can lack meaningful content and are prone to producing nonsensical phrases.\n",
        "\n",
        ">**Example**  \n",
        ">**Question:** *D√©cris les diff√©rences entre le mod√®le GPT-2 et le mod√®le GPT-3*  \n",
        ">**Answer:** *Le mod√®le GPT-2 est un mod√®le de langage g√©n√©r√© d√©velopp√© par OpenAI. Il est basse de pays ose ter voliance par le cad de pays membre longue de bell. Ass end quotid men endse animerie longues de nombreonde extrautes vert endale du mus√©e nume numaraccent.*\n",
        "\n",
        "While the grammar and structure seem correct, the generated response drifts into gibberish, showcasing the trade-offs between model size and capability. This highlights why larger models are often necessary for more accurate and contextually aware language generation, as they have more capacity to represent complex relationships within the data.\n",
        "\n",
        "In the next section, a fine-tuned LLM will be used to explore how training on domain-specific data can significantly improve the quality of responses. Let‚Äôs take this learning further! üöÄ"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
